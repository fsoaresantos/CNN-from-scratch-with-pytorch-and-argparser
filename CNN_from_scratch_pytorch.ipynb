{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN from scratch-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcPr7cmzG2nPXSXtlaeHw+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsoaresantos/CNN-from-scratch-with-pytorch-and-argparser/blob/main/CNN_from_scratch_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN from scratch with `pytorch` and `argparser`"
      ],
      "metadata": {
        "id": "N40JVKZHP-iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Notebook is composed of two parts:\n",
        "\n",
        "The first writes down (with `%%writefile`) the content of the actual Notebook cell to the python script (parsing.py). The script will then contains the code to the CNN model definition, the functions to train and test the model, and the main function with the definition of the command-line arguments (with `argparser` module), the code to download, transform and the parallelize the CIFAR10 dataset, the creation of the model and the trainning loop.\n",
        "\n",
        "The second part calls the script with any arguments it accepts and therefore is what makes the code run.\n",
        "\n",
        "**NOTES:**\n",
        "\n",
        "If any issues with torch.flatten(), in the function `def forward()` of `class Net`, and/or with the parameter reduction of `F.nll_loss(..., reduction=\"sum\")`, in the function `def test(...)`, Two solutions are possible:\n",
        "\n",
        "1) update torch and torchvision with the command:\n",
        "\n",
        "$ python -m pip install torch==1.10.2+cu102 torchvision==0.11.3+cu102 torchaudio===0.10.2+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html\n",
        "\n",
        "OR\n",
        "\n",
        "2) opt to use older versions of these functions (if, for instance, torch.__version__ < 0.4.1):\n",
        "* use `x = x.view(x.size()[0],x.size()[1]*x.size()[2]*x.size()[3])` instead of `torch.flatten()`\n",
        "* use `test_loss += F.nll_loss(output, target, size_average=False).item()` to calculate the test loss"
      ],
      "metadata": {
        "id": "aQiFnOKswycR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WRITTING THE SCRIPT"
      ],
      "metadata": {
        "id": "fV8mDBxFQvT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parsing.py\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=9, kernel_size=(3,3), stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=9, out_channels=32, kernel_size=(3,3), stride=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), stride=1)\n",
        "        self.fc1 = nn.Linear(in_features=64*4*4, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=56)\n",
        "        self.fc3 = nn.Linear(in_features=56, out_features=10)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ##print(x.shape) #######\n",
        "        x = F.relu(self.conv1(x))\n",
        "        ##print(x.shape) #######\n",
        "        x = self.pool1(x)\n",
        "        ##print(x.shape) #######\n",
        "        x = F.relu(self.conv2(x))\n",
        "        ##print(x.shape) #######\n",
        "        x = self.pool2(x)\n",
        "        ##print(x.shape) #######\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #print(x.shape) ####### print out the torch.size for in_features of self.fc1\n",
        "        #x = x.view(x.size()[0],x.size()[1]*x.size()[2]*x.size()[3]) #if torch.__version__ < 0.4.1\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        model_str = F.log_softmax(x, dim=1)\n",
        "        return model_str\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
        "            #test_loss += F.nll_loss(output, target, size_average=False).item() #if torch.__version__ < 0.4.1\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
        "        )\n",
        "    )\n",
        "\n",
        "def main():\n",
        "    # define the argumentparser\n",
        "    parser = argparse.ArgumentParser(description=\"define hyperparameters for CNN PyTorch CIFAR10 Example\")\n",
        "\n",
        "    # add command-line arguments (for model hyperparameters)\n",
        "    parser.add_argument(\n",
        "        \"--batch_size\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        metavar=\"N\",\n",
        "        help=\"training batch size (default: 64)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test_batch_size\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"test batch size (default: 1000)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 5)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=3e-1,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 3e-1)\"\n",
        "    )\n",
        "\n",
        "    # determine the end of arguments definition\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # download the CIFAR10 dataset and create data loaders\n",
        "    trainset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    testset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=False)\n",
        "\n",
        "    # create/call the model\n",
        "    model = Net()\n",
        "\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        images, labels = next(iter(train_loader))\n",
        "        logps = model(images)\n",
        "    \"\"\"\n",
        "\n",
        "    # print model we just instantiated\n",
        "    print(model)\n",
        "\n",
        "    # define optimizer criteria\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train(model, train_loader, optimizer, epoch)\n",
        "        test(model, test_loader)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OzYjjsc3nxwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97584c0-0edc-4f01-87c6-0ba0ca9bc5d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parsing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUNNING THE SCRIPT"
      ],
      "metadata": {
        "id": "I1W6qPuYqD0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## e.g. 1) calling script and passing a new value to 'test_batch_size' argument\n",
        "!python3 parsing.py --test_batch_size 64\n",
        "\n",
        "\"\"\"\n",
        "batch size: 64\n",
        "test batch size: 64\n",
        "number of epochs: 10\n",
        "learning rate: 0.3\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "StgzL06rquZu",
        "outputId": "070e604a-953a-4ca9-be3e-22fdc470482d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "170499072it [00:03, 43012366.55it/s]                   \n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Net(\n",
            "  (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=1024, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=56, bias=True)\n",
            "  (fc3): Linear(in_features=56, out_features=10, bias=True)\n",
            ")\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.301338\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.180056\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.964926\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.034715\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.524465\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.758923\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.647512\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.563411\n",
            "\n",
            "Test set: Average loss: 1.6940, Accuracy: 3824/10000 (38%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.565776\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.216501\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.258118\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.476908\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.408234\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.316697\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.584348\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.528433\n",
            "\n",
            "Test set: Average loss: 1.3995, Accuracy: 5037/10000 (50%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.469898\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.327823\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.401119\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.409715\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.340983\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.317424\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.356759\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.140700\n",
            "\n",
            "Test set: Average loss: 1.2471, Accuracy: 5524/10000 (55%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.302465\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.189295\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.154740\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.255290\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.064387\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.977842\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.113639\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.198633\n",
            "\n",
            "Test set: Average loss: 1.2690, Accuracy: 5487/10000 (55%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.095104\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.964713\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.133317\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.007927\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.954700\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.920951\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.981281\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.904400\n",
            "\n",
            "Test set: Average loss: 1.1095, Accuracy: 6071/10000 (61%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.023936\n",
            "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 1.201036\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 1.031047\n",
            "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.694365\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.891824\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.110228\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.812875\n",
            "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.872897\n",
            "\n",
            "Test set: Average loss: 1.0259, Accuracy: 6453/10000 (65%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.807327\n",
            "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.870371\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 1.210598\n",
            "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.739222\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.080694\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.987454\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.638484\n",
            "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.847108\n",
            "\n",
            "Test set: Average loss: 1.1788, Accuracy: 5854/10000 (59%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.942413\n",
            "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 1.158303\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.750183\n",
            "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.798418\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.808742\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.815913\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.914415\n",
            "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.725442\n",
            "\n",
            "Test set: Average loss: 1.0221, Accuracy: 6572/10000 (66%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.892790\n",
            "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.733442\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.892343\n",
            "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.712201\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.843288\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.631097\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.831334\n",
            "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.570070\n",
            "\n",
            "Test set: Average loss: 1.0123, Accuracy: 6566/10000 (66%)\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.615819\n",
            "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.746202\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.434608\n",
            "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.524118\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.548177\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.729493\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.692222\n",
            "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.634464\n",
            "\n",
            "Test set: Average loss: 1.0538, Accuracy: 6453/10000 (65%)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbatch size: 64\\ntest batch size: 64\\nnumber of epochs: 10\\nlearning rate: 0.3\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## e.g. 2) calling the script with default arguments values\n",
        "!python3 parsing.py\n",
        "\n",
        "\"\"\"\n",
        "batch size: 64\n",
        "test batch size: 1000\n",
        "number of epochs: 10\n",
        "learning rate: 0.3\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ndO5qFuHrWt7",
        "outputId": "90ae548a-4211-49f6-9068-1a31f18ed750"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Net(\n",
            "  (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=1024, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=56, bias=True)\n",
            "  (fc3): Linear(in_features=56, out_features=10, bias=True)\n",
            ")\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.299312\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.225833\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.919918\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.795488\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.694204\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.828414\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.539762\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.577595\n",
            "\n",
            "Test set: Average loss: 1.5852, Accuracy: 4057/10000 (41%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.644061\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.819409\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.688761\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.212186\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.198103\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.486978\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.301573\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.413854\n",
            "\n",
            "Test set: Average loss: 1.5263, Accuracy: 4645/10000 (46%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.585994\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.281357\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.430384\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.131729\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.372374\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.332340\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.105381\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.365213\n",
            "\n",
            "Test set: Average loss: 1.2519, Accuracy: 5558/10000 (56%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.238262\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.232034\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.285701\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.977612\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.066747\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.097336\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.117501\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.194302\n",
            "\n",
            "Test set: Average loss: 1.2492, Accuracy: 5538/10000 (55%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.076741\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.140817\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.136483\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.200859\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.980233\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.831758\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.927040\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.311919\n",
            "\n",
            "Test set: Average loss: 1.2780, Accuracy: 5538/10000 (55%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.090473\n",
            "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 1.139717\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.941211\n",
            "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.806076\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.014631\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.740924\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.970901\n",
            "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.894604\n",
            "\n",
            "Test set: Average loss: 1.1003, Accuracy: 6126/10000 (61%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.018205\n",
            "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 1.089994\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.923753\n",
            "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 1.212450\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.033440\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.138046\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.757386\n",
            "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.817573\n",
            "\n",
            "Test set: Average loss: 1.0727, Accuracy: 6210/10000 (62%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.810419\n",
            "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.873659\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.860524\n",
            "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 1.053545\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.860588\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.001678\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.892325\n",
            "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 1.060310\n",
            "\n",
            "Test set: Average loss: 1.0635, Accuracy: 6326/10000 (63%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.964916\n",
            "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.629895\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.601521\n",
            "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.491688\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.774610\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.651866\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.841251\n",
            "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.474230\n",
            "\n",
            "Test set: Average loss: 1.0239, Accuracy: 6460/10000 (65%)\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.862757\n",
            "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.720950\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.789075\n",
            "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.725233\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.811474\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.964666\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.376314\n",
            "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.858854\n",
            "\n",
            "Test set: Average loss: 1.0755, Accuracy: 6428/10000 (64%)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbatch size: 64\\ntest batch size: 1000\\nnumber of epochs: 10\\nlearning rate: 0.3\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}